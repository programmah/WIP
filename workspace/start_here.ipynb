{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b3b1b9-32fd-459d-a8df-b7820017b8f8",
   "metadata": {},
   "source": [
    "# Distributed Model Training\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3cf9d-7648-4bf6-a4c2-fe75580ace6e",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The Distributed Model Training Bootcamp is designed from a real-world perspective on how to efficiently utilize GPUs in training models in a distributed manner. Attendees walk through the system topology to learn the dynamics of multi-GPU and multi-node connections and architecture. Using the PyTorch Framework, they will also learn and understand state-of-the-art strategies for training models that include distributed data parallelism (DDP), Fully Sharded Data Parallelism (FSDP), model parallelism, pipeline parallelism, and tensor parallelism. Furthermore, attendees will learn to profile code and analyze performance using NVIDIA® Nsight™ Systems. This tool helps identify optimization opportunities and improve the performance of applications running on a system consisting of multiple CPUs and GPUs.\n",
    "\n",
    "\n",
    "### Why Distributed Training?\n",
    "\n",
    "Training deep learning models is a task that often takes a long time because the process typically requires substantial storage and computing capacity. During training, intermediate results must be calculated and held in memory. Therefore, dividing one huge task into a number of subtasks and running them parallelly makes the whole process much more time efficient and enables us to complete complex tasks with large datasets. In distributed training, storage and compute power are magnified, reducing training time. Distributed Model Training speeds up training and enables the training of very large models through different strategies listed in the overview section.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b120976-4f29-401b-a01a-2f8ca2cad178",
   "metadata": {},
   "source": [
    "The table of contents below will walk you through the topology of a multi-GPU/multi-node environment, solidifying your understanding of various distributed model training strategies, as well as performance.\n",
    "\n",
    "### Table of Content\n",
    "\n",
    "The following contents will be covered:\n",
    "\n",
    "- Lab 1: [System Topology](jupyter_notebook/system-topology.ipynb)\n",
    "- Lab 2: Distributed Training Strategy\n",
    "    1. [Data Parallelism](jupyter_notebook/data-parallelism.ipynb)\n",
    "    3. [Fully Sharded Data Parallelism (FSDP)](jupyter_notebook/fsdp.ipynb)\n",
    "    4. [Model Parallelism](jupyter_notebook/model-parallelism.ipynb)\n",
    "    5. [Pipeline Parallelism](jupyter_notebook/pipeline-parallelism.ipynb)\n",
    "    6. [Tensor Parallelism](jupyter_notebook/tensor-parallelism.ipynb)\n",
    "    7. [Message Passing and Mixed Precision](jupyter_notebook/other-topics.ipynb)\n",
    "- Lab 3: Performance Overview\n",
    "    1. [Profiling DDP with Nsight Systems](jupyter_notebook/nsys-introduction.ipynb)\n",
    "\n",
    "\n",
    "### Tutorial Duration\n",
    "\n",
    "The material will be presented in a total of 8hrs\n",
    "\n",
    "### Content Level\n",
    "\n",
    " Advanced\n",
    "\n",
    "### Target Audience and Prerequisites\n",
    "\n",
    "The target audience for these labs is researchers, graduate students, and developers interested in scaling their model training approach to solving tasks via GPUs. Audiences are expected to have background Knowledge of Python programming and the PyTorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fc284-1f61-4aab-87a5-f4f1355ea2c2",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c316c2-23b4-4fdb-a0b8-4399a32bf869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
