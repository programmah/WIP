{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7977cb-001e-4e5f-b974-04856b3236dd",
   "metadata": {},
   "source": [
    "# Message Passing and Mixed Precision\n",
    "---\n",
    "\n",
    "The objective of this notebook is to expose you briefly to the concept of message-passing and mixed precision. The content also includes the implementations to aid your understanding.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8e8ca-6a0b-45b3-be34-b57f7ec18ccc",
   "metadata": {},
   "source": [
    "## Message Passing\n",
    "\n",
    "`torch.distributed` package enables parallelism across processes and clusters of machines. It leverages `message passing` semantics that allows data communication among processes with different communication backends and machines. The message passing can be executed by running multiple processes simultaneously or using a single machine to spawn multiple processes. There are two communication approaches to consider, `Point-to-Point` and `Collective` communications.\n",
    "\n",
    "\n",
    "#### Point-to-Point Communication\n",
    "\n",
    "Point-to-point communication is a data transfer from one process to another through `send` and `recv` functions or immediate counter-parts, `isend` and `irecv.` \n",
    "\n",
    "<img src=\"images/send_recv.png\" width=\"500px\" height=\"500px\" alt-text=\"p2p\"/>\n",
    "\n",
    "The communication pattern for `send/recv` can be blocking or non-blocking. When both processes are blocked until communication is completed, it is called blocking, but when they continue to execute and possess a `DistributedRequest` object upon which we can choose to `req.wait()`, it is referred to as non-blocking. Point-to-point communication is helpful when we want more fine-grained control over the communication of our processes. Both patterns can be implemented using fancy algorithms, like in Baidu’s [DeepSpeech](https://github.com/baidu-research/baidu-allreduce) or Facebook’s [large-scale experiments](https://research.facebook.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/). In the blocking point-to-point communication code snippet below, both processes start with a zero tensor, then `process 0` increments the tensor and sends it to `process 1` to both possess a value of 1.0. However, `process 1` needs to allocate memory to store the data received. \n",
    "\n",
    "\n",
    "**Blocking point-to-point communication**\n",
    "```python\n",
    "\"\"\"source: https://pytorch.org/tutorials/intermediate/dist_tuto.html\"\"\"\n",
    "\n",
    "def run(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "**Non-blocking point-to-point communication**\n",
    "\n",
    "Running the function below might result in `process 1` still having `0.0` while having already started receiving. However, after `req.wait()` has been executed, we are guaranteed that the communication took place and that the value stored in `tensor[0]` is 1.0.\n",
    "\n",
    "```python\n",
    "\"\"\"source: https://pytorch.org/tutorials/intermediate/dist_tuto.html\"\"\"\n",
    "\n",
    "def run(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13143ac5-dc3e-433d-bf1e-d3408f3a1935",
   "metadata": {},
   "source": [
    "#### Collective Communication\n",
    "\n",
    "Collective communication allows communication patterns across all processes in a **group**. A group is a subset of all processes. To create a group, we can pass a list of ranks to `dist.new_group(group)`. By default, collectives are executed on all processes, also known as the **world**. For example, to obtain the sum of all tensors on all processes, use the `dist.all_reduce(tensor, op, group)` collective. An All-Reduce example is given below:\n",
    "\n",
    "```python\n",
    "\"\"\" source: https://pytorch.org/tutorials/intermediate/dist_tuto.html\"\"\"\n",
    "\n",
    "def run_all_reduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1]) \n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "```\n",
    "\n",
    "To sum up all tensors in the group, use `dist.reduce_op.SUM` as the reduce operator. Generally, any commutative mathematical operation can be used as an operator. PyTorch comes with four such operators, and all are executed element-wise:\n",
    "\n",
    "* `dist.reduce_op.SUM`,\n",
    "* `dist.reduce_op.PRODUCT`,\n",
    "* `dist.reduce_op.MAX`,\n",
    "* `dist.reduce_op.MIN`.\n",
    "\n",
    "In addition to `dist.all_reduce(tensor, op, group)`, there are a total of 6 collectives (*broadcast, reduce, all_reduce, scatter, gather, and all_gather*) currently implemented in PyTorch. Details on them with diagram illustration were discussed in the system topology [notebook](system-topology.ipynb) \n",
    "\n",
    "Let's execute the [message passing send-recv script](../source_code/send_receive.py) by spawning two processes to setup the distributed environment, initialize the process group (`dist.init_process_group`), and finally execute the given `run` function. The `init_processes` ensures that every process will be able to coordinate through a master, using the same `ip address` and `port.`\n",
    "\n",
    "Let's run the `Blocking point-to-point` communication part of our script with two GPUs by executing the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1ac37-7a27-4cf3-aeb4-37178d50fddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:2 python3 send_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf6964-f6a1-4aad-9862-3bfd260c9e6c",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "Rank 0 started sending\n",
    "Rank  0  has data  tensor(1.)\n",
    "Rank 1 started receiving\n",
    "Rank  1  has data  tensor(1.)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f6dbfe6-bf5f-417d-addd-265818ac8361",
   "metadata": {},
   "source": [
    "## Overview of Mixed Precision \n",
    "\n",
    "[Mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) is the combined use of different numerical formats (single- and half-precision computation) in the training of a deep neural network. The single precision is referred to as `FP32 (float32),` while the half-precision is denoted as `FP16 (float16).` Mixed precision training offers significant computational speedup by performing operations in half-precision format while storing minimal information in single-precision to retain as much information as possible in critical parts of the network. Since introducing [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/) in GPU architectures, significant training speedups have been experienced by switching to mixed precision. Using mixed precision training requires two steps: `Porting the model to use the FP16 data type where appropriate` and `adding loss scaling to preserve small gradient values`.\n",
    "\n",
    "#### Benefits of Mixed Precision Training\n",
    "\n",
    "- Speeds up math-intensive operations, such as linear and convolution layers, by using [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/).\n",
    "- Speeds up memory-limited operations by accessing half the bytes compared to single-precision.\n",
    "- Reduces memory requirements for training models, enabling larger models or larger minibatches.\n",
    "\n",
    "#### FP16 Porting\n",
    "\n",
    "- FP16 dynamic range is sufficient for training; however, gradients may require scaling to move them into the range to keep them from becoming zeros in FP16. Overflow should be avoided.\n",
    "\n",
    "  <img src=\"images/mixed-precision.png\" width=\"450px\" height=\"450px\" alt-text=\"mp\"/>\n",
    "  \n",
    "- Loss Scaling: The purpose of loss scaling is to preserve small gradient magnitudes. \n",
    "\n",
    "*Training procedure:*\n",
    "```text\n",
    "1. Maintain a primary copy of weights in FP32\n",
    "2. For each iteration:\n",
    "     Make an FP16 copy of the weights\n",
    "     Forward propagation (FP16 weights and activations)\n",
    "     Multiply the resulting loss with the scaling factor S\n",
    "     Backward propagation (FP16 weights, activations, and their gradients)\n",
    "     Multiply the weight gradient with 1/S\n",
    "     Complete the weight update (including gradient clipping)\n",
    "```\n",
    "- Procedure for choosing a scaling factor\n",
    "\n",
    "```text\n",
    "1. Maintain a primary copy of weights in FP32.\n",
    "2. Initialize S to a large value.\n",
    "3. For each iteration:\n",
    "     Make an FP16 copy of the weights.\n",
    "     Forward propagation (FP16 weights and activations).\n",
    "     Multiply the resulting loss with the scaling factor S.\n",
    "     Backward propagation (FP16 weights, activations, and their gradients).\n",
    "     If there is an Inf or NaN in weight gradients:\n",
    "         Reduce S.\n",
    "         Skip the weight update and move to the next iteration.\n",
    "     Multiply the weight gradient with 1/S.\n",
    "     Complete the weight update (including gradient clipping, etc.).\n",
    "     If there hasn’t been an Inf or NaN in the last N iterations, increase S.\n",
    "\n",
    "```\n",
    "**Summary of Mixed Precision Training**\n",
    "- Choose FP16 format tensor core\n",
    "- Forward pass of the model\n",
    "- Scale the loss and backpropagate the scaled gradients\n",
    "- Un-scale the gradients and optimizer performs the weight update\n",
    "\n",
    "**Automatic Mixed Precision (AMP)**\n",
    "\n",
    "[Automatic Mixed Precision (AMP)](https://developer.nvidia.com/automatic-mixed-precision) makes mixed precision training with FP16 easy in frameworks. AMP automates the process of training in mixed precision. It converts matrix multiplies/convolutions to 16-bits for Tensor Core acceleration.\n",
    "\n",
    " <img src=\"images/amp.png\" width=\"550px\" height=\"550px\" alt-text=\"amp\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eb78241-e44b-4e77-9e2f-5d31f2bdd08b",
   "metadata": {},
   "source": [
    "### Pytorch Automatic Mixed Precision\n",
    "\n",
    "In Pytorch, automatic mixed precision training implies training with `torch.autocast` and `torch.amp.GradScaler` together. Instances of `torch.autocast` enable autocasting for chosen regions. Autocasting automatically chooses the precision for operations to improve performance while maintaining accuracy. Instances of `torch.amp.GradScaler` help perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with float16 (by default on CUDA and XPU) gradients by minimizing gradient underflow, as explained [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling).\n",
    "\n",
    "Below, we illustrate the process using AMP. Using default precision (without torch.cuda.amp) implies that all ops are executed in default precision (torch.float32):\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Default precision:\")\n",
    "...\n",
    "```\n",
    "**Adding torch.autocast**\n",
    "\n",
    "Instances of `torch.autocast` serve as context managers that allow script regions to run in mixed precision. In these regions, `CUDA` ops run in a `dtype` chosen by `autocast` to improve performance while maintaining accuracy. For details on what precision `autocast` chooses for each op, See the [autocast Op reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference).\n",
    "\n",
    "```python\n",
    "for epoch in range(0): # 0 epochs, this section is for illustration only\n",
    "    for input, target in zip(data, targets):\n",
    "        # Runs the forward pass under ``autocast``.\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            output = net(input)\n",
    "            # output is float16 because linear layers ``autocast`` to float16.\n",
    "            assert output.dtype is torch.float16\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            # loss is float32 because ``mse_loss`` layers ``autocast`` to float32.\n",
    "            assert loss.dtype is torch.float32\n",
    "\n",
    "        # Exits ``autocast`` before backward(). Backward passes under ``autocast`` are not recommended. Backward ops run in the same ``dtype`` ``autocast`` chose for corresponding forward ops.\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "```\n",
    "**Adding GradScaler**\n",
    "\n",
    "Gradient scaling helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.\n",
    "\n",
    "```python\n",
    "# Constructs a ``scaler`` once, at the beginning of the convergence run, using default arguments.\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "for epoch in range(0): # 0 epochs, this section is for illustration only\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        # Scales loss. Calls ``backward()`` on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "# ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters. If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called; otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(opt)\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "```\n",
    "\n",
    "**Complete Flow for Automatic Mixed Precision**\n",
    "\n",
    "In the example below, an optional argument `enabled` is added. If set to false,  `autocast` and `GradScaler‘s` calls become no-ops. This allows switching between default and mixed precision without if/else statements.\n",
    "```python\n",
    "use_amp = True\n",
    "...\n",
    "scaler = torch.amp.GradScaler(\"cuda\" ,enabled=use_amp)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Mixed precision:\")\n",
    "\n",
    "```\n",
    "You can learn more about modifying gradients (e.g., clipping), saving/resuming Amp-enabled runs with bitwise accuracy, and inference/Evaluation through this [link](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#inspecting-modifying-gradients-e-g-clipping). \n",
    "\n",
    "Please run the cell below to execute a [sample DDP](../source_code/ddp_mixed_precision.py) with the application of AMP using 2 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663130c-8e5a-4e71-900b-f4f5b7cd2aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:2 torchrun --nnodes 1 --nproc_per_node 2 ddp_mixed_precision.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cad241-654e-44b0-b59d-9999eff0c565",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```python\n",
    "W0227 13:26:29.388000 2903532 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "W0227 13:26:29.388000 2903532 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
    "W0227 13:26:29.388000 2903532 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "[rank0]:[W227 13:26:32.794582922 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. \n",
    "...\n",
    "Start training...\n",
    "[Epoch 1/4] loss: 2.834\n",
    "[Epoch 2/4] loss: 2.134\n",
    "[Epoch 3/4] loss: 1.973\n",
    "[Epoch 4/4] loss: 1.866\n",
    "Finished Training\n",
    "...\n",
    "```\n",
    "\n",
    "Now, we have come to the end of the distributed training strategy. We can proceed to the next notebook to learn how to profile, trace bottlenecks, and improve performance using `NVIDIA Nsight Systems.` Please click the [Next Link](nsys-introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99ac24-cad1-47c1-b8da-710aa7ddcc60",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
    "- https://github.com/programmah/hpdl/blob/multi_gpu_pytorch/Pytorch_Distributed_Deep_Learning/workspace/jupyter_notebook/07-Message_Passing.ipynb\n",
    "- https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html\n",
    "- https://developer.nvidia.com/automatic-mixed-precision\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
