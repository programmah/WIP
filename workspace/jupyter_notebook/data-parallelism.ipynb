{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a4161b-4555-4a07-bd1e-442259eb7cef",
   "metadata": {},
   "source": [
    "# Distributed Data Parallelism\n",
    "---\n",
    "\n",
    "In this notebook, you learn and understand the concept of distributed data parallelism. The content will also work you through various approaches to distributed data parallel training, which include stochastic gradient descent, model synchronization, hyperparameter tuning, parameter server architecture, and All-Reduce. Lastly, you will also be able to learn how to implement these concepts with `torchrun` and `slurm` using `multi-GPU` and `multi-node` computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa46e4-a09c-49bf-a8d2-894a44ef538e",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The standard practice for speeding up the model training process is parallel execution. The most popular in-parallel model training is called `data parallelism`. In data parallel training, each GPU/node holds the full copy of a model. Then, it partitions the input data into disjoint subsets, where each GPU/node is only responsible for model training on one of the input partitions. Since each GPU only trains its local model on a subset (not the whole set) of the input data, we need to conduct a procedure called model synchronization periodically. Model synchronization ensures that, after each training iteration, all the GPUs involved in this training job are on the same page. This guarantees that the model copies that are held on different GPUs have the same parameter values.  `Data parallelism` can also be applied at the model serving stage. Given that the fully trained model may need to serve many inference tasks, splitting the inference input data can also reduce the end-to-end model serving time. One significant difference compared to data parallel training is that in data parallel inference, all the GPUs/nodes involved in a single job do not need to communicate anymore, which means that the model synchronization phase during data parallel training is completely removed. \n",
    "\n",
    "### Fundamentals of Data Parallelism\n",
    "\n",
    "Let‚Äôs examine some fundamental theories about data parallel training, such as `stochastic gradient descent (SGD)` and `model synchronization`. But before that, let's examine the system architecture for data-parallel training. \n",
    "\n",
    "<center><img src=\"images/data-parallel-training-workflow.png\" width=\"350px\" height=\"350px\" alt-text=\"workflow\"/></center>\n",
    "<center>Simplified workflow of data parallel training</center></br>\n",
    "\n",
    "The simplified workflow for data-parallel training is depicted in the diagram above, emphasizing two bandwidths (the `data loading bandwidth` and the `model training bandwidth`). As we can see, the main difference between single-GPU training and data parallel training is that we split the data loading bandwidth between multiple workers/GPUs (shown as blue arrows in the diagram). In data parallel training, different batches of input data are trained on different GPUs. Consequently, none of the GPUs can see the full training data. Thus, traditional gradient descent optimization cannot be applied here. A stochastic approximation of gradient descent is needed, which can be used in the single-GPU case. One popular stochastic approximation method is SGD. Also, besides the three usual steps in sing-GPU training (data loading, training, and model update), data-parallel training includes an additional step called `model synchronization,` a concept that collects and aggregates local gradients that have been generated by different GPUs/nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5141a-fcda-47d1-8bff-9be31f2a43b9",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent (SGD)\n",
    "\n",
    "Theoretically, traditional gradient descent (GD) for single-GPU training can be estimated by calculating the gradients from each data point of the training dataset, where `g_i` is the gradient. The `i-th` training data point can be calculated as follows: \n",
    "`g_i = dL(w_i) / dw`  \n",
    "Then, we sum up all the gradients that have been calculated by all the training data points (g_all += g_i) and then do a single-step model update with `w = w - a*g_all` \n",
    "\n",
    "```python\n",
    "\n",
    "for i in dataset:\n",
    "    g_all += g_i\n",
    "    w = w - a*g_all \n",
    "```\n",
    "However, in data parallel training, each GPU can only see part of (not the full) training dataset, which makes traditional GD optimization impossible since we cannot calculate g_all in this case. Thus SGD is used. With SGD, instead of updating the model weights (w) after generating the gradients from all the training data, SGD allows for model weight updates using a single or a few training samples (for example, a mini-batch). With this, workers in data-parallel training can update their model weights using their local (not global) training samples. \n",
    "\n",
    "```python\n",
    "for i in dataset:\n",
    "    w = w - a*g_i \n",
    "```\n",
    "\n",
    "This implies that the model parameters of different workers can differ after each training iteration. Therefore, periodic `model synchronization` is needed to guarantee that all the workers are on the same page, meaning that they maintain the model parameters after each training iteration. \n",
    "\n",
    "#### Model synchronization \n",
    "\n",
    "Model synchronization is needed to force all the workers to have the same view of the model parameters. Let's consider a simple four-GPU setting in a data-parallel training job. Each GPU maintains a copy of the full ML model locally inside its on-device memory. Let's assume all the GPUs are initialized with the same model parameters. After the first training iteration, each GPU will generate its local gradients as `‚àáùëäùëñ`, where `i` refers to the`i-th` GPU. Given that they are training on different local training inputs, all the gradients from different GPUs may differ. To guarantee that all four GPUs have the same model updates, we need to conduct model synchronization before the model parameter updates. Model synchronization does two things: \n",
    "\n",
    "- Collects and sums up all the gradients from all the GPUs in use, as shown here:\n",
    "\n",
    "```python\n",
    "\n",
    "  ‚àáùëä = ‚àáùëä1 + ‚àáùëä2 + ‚àáùëä3+ . . . +‚àáùëäùëÅ \n",
    "```\n",
    "  \n",
    "- Broadcasts the aggregated gradients to all the GPUs\n",
    "\n",
    "Then, we can use these aggregated gradients, ‚àáùëä, for the model updates, which guarantees that the updated model parameters remain the same after this first data parallel training iteration. This process is repeated to guarantee that the model parameters remain the same after every training iteration in a particular data parallel training job.  Empirically, this model synchronization mainly has two different variations for the real system implementations: the `parameter server architecture` and the `All-Reduce architecture.` \n",
    "\n",
    "#### Hyperparameter tuning \n",
    "\n",
    "- Global batch size: The global batch size refers to how many training samples will be loaded into all the GPUs for training simultaneously. In data parallel training, this global batch size is the first hyperparameter that must be searched or fine-tuned. If the global batch size is too large, the training model may not converge. If it is too small, it is just a waste of distributed computational resources.\n",
    "  \n",
    "- Learning rate adjustment: Recent research suggests that, for large-batch data parallel training, we should have a warmup stage at the beginning of the training stage. This warmup policy suggests that we start data parallel training with a relatively small learning rate. After this warmup period, we should gradually increase the learning rate for several epochs of training and then stop increasing it by defining a peak learning rate.\n",
    "\n",
    "- Model synchronization schemes: This scheme is required to initialize a group of processes to run our data parallel training job in a distributed manner. Each process will be responsible for handling model synchronization on one machine or one GPU.  A PyTorch example to initialize a process group using model synchronization backend is as follows:\n",
    "\n",
    "```python\n",
    "\ttorch.distributed.init_process_group(backend='nccl', init_method = '...', world_size = N, timeout = M) \n",
    "```\n",
    "PyTorch mainly supports three different communication backends: `NCCL (GPU only), Gloo (CPU and partial support GPU), and MPI (CPU only)`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec13e2e-6d63-4ccc-8bc1-853ef1381cdf",
   "metadata": {},
   "source": [
    "### Parameter Server Architecture\n",
    "\n",
    "The summary of the `parameter server` paradigm is that model parameters are kept in one place (a centralized node). Whenever a GPU/node needs to conduct model training, it pulls the parameters from the centralized node, trains the model, and then pushes back model updates to the centralized node. Model consistency is guaranteed since all the GPUs/nodes pull from the same centralized node. The `parameter server architecture` mainly consists of two roles: `parameter server` and `worker`. The `parameter server` is the master node in the traditional Master/Worker architecture. Workers are the computer nodes or GPUs responsible for model training. The total training data is split among all the workers. Each worker trains its local model with the assigned training data partition. The duties of the parameter server are twofold:\n",
    "\n",
    "- Aggregate model updates from all the workers.\n",
    "- Update the model parameters held on the parameter server. \n",
    "\n",
    "<center><img src=\"images/parameter-server-arch.png\" width=\"350px\" height=\"350px\" alt-text=\"workflow\"/></center>\n",
    "\n",
    "The screenshot above is a simplified parameter server architecture with two workers and one parameter server in the system. The whole system works through the following four stages: \n",
    "\n",
    "1. Pull Weights: All the workers pull the model parameters/weights from the centralized parameter server.\n",
    "1. Push Gradients: Each worker trains its local model with its local training data partition and generates local gradients. Then, all the workers push their local gradients to the centralized parameter server.\n",
    "1. Aggregate Gradients: After collecting all the gradients sent from the worker nodes, the parameter server sums up all the gradients.\n",
    "1. Model Update: Once the aggregated gradients have been calculated, the parameter server uses them to update the model parameters on this centralized server.\n",
    "\n",
    "These four steps are executed among the parameter server and workers for each training iteration and looped through for the whole model training process. However, the communication (pull weights and push gradients) in the parameter server architecture can often be the training bottleneck. Details on this are beyond the scope of this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5c436-1a1c-4d0a-a102-f64fd6fa20d7",
   "metadata": {},
   "source": [
    "### All-Reduce Architecture \n",
    "\n",
    "The idea of the All-Reduce paradigm is that every GPU/node keeps a copy of the model parameters so the model copies are forced to synchronize periodically. Each GPU trains its local model replica using its own training data partition. After each training iteration, the model replicas that are held on different GPUs can be different since they are trained with different input data. Therefore, a global synchronization step is injected after each training iteration. This averages the parameters that are held on different GPUs so that model consistency can be guaranteed in this fully distributed manner. In the `All-Reduce` Architecture, every node/GPU is equivalent, and all of them are worker nodes/GPUs. The burden of implementing communication protocols is left to standard collective communication libraries like NCCL. The All-Reduce paradigm is borrowed from the traditional Message Passing Interface (MPI) domain. Before illustrating All-Reduce, let‚Äôs briefly look at the Reduce collective primitive. The Reduce operator `(+)` is used to aggregate the values from different nodes/GPUs and store them in a single node/GPU. That is, there is a node/GPU that maintains the aggregated value. Some of the most common Reduce operators are `Sum,` `Averaging,` and `Multiplication.` \n",
    "\n",
    "<center><img src=\"images/all-reduce-primitive.png\" width=\"300px\" height=\"300px\" alt-text=\"all-reduce\"/></center>\n",
    "<center>The All-Reduce primitive in a three-worker setting<center/><br/>\n",
    "    \n",
    "All-Reduce allows all the nodes/GPUs to get the same aggregated value. The All-Reduce function allows all the workers to get the aggregated gradients from all the worker nodes. This gradient aggregation is the model synchronization procedure in the All-Reduce architecture. It guarantees that all the workers use the same gradient to update the model in the current training iteration. All-Reduce uses all-to-all communication. Every worker needs to send their value to all the other workers. The All-reduce function is implemented as `Ring All-Reduce.` Ring All-Reduce has been implemented using `NVIDIA NCCL,` `Uber Horovod,` and `Facebook Gloo.` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799843f9-1b0d-4fe4-a3f9-4665b2a06302",
   "metadata": {},
   "source": [
    "#### Ring All-Reduce Illustration\n",
    "\n",
    "For simplicity purposes, let‚Äôs illustrate with three workers as follows.\n",
    "\n",
    "<center><img src=\"images/RingAll-reduce.png\" width=\"700px\" height=\"700px\" alt-text=\"all-reduce\"/></center>\n",
    "\n",
    "\n",
    "- **Step 1**: `Worker 1` has a value of `a`, `Worker 2` has a value of `b`, and `Worker 3` has a value of `c.`  \n",
    "- **Step 2**: `Worker 1` has a value of `a`. `Worker 1` passes this value, `a`, to `Worker 2`. `Worker 2` gets `a+b.` `Worker 3` still has a value of `c.`\n",
    "- **Step 3**: `Worker 1` has a value of `a`. `Worker 2` has a value of `a+b,` which it passes to `Worker 3.` `Worker 3` now has a value of `a+b+c.`\n",
    "- **Step 4**: `Worker 3` passes `a+b+c` to `Worker 1`. `Worker 1` now has `a+b+c.` `Worker 2` now has `a+b`. `Worker 3` now has `a+b+c.`\n",
    "- **Step 5**: `Worker 1,` who has `a+b+c,` passes `a+b+c` to `Worker 2.` `Worker 2` now has `a+b+c,` and `Worker 3` has `a+b+c` as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caba469-ba08-4504-9c42-6e44992e8f7e",
   "metadata": {},
   "source": [
    "### Building a Data Parallel Training\n",
    "\n",
    "This section walks you through the process of implementing data parallelism using two different settings: single-machine multi-GPU and multi-machine/nodes multi-GPU. Also, we will illustrate how to checkpoint the model and its relevant metadata during training. We will focus on using a simple CNN as our model and MNIST as our dataset. The data parallel training pipeline of each worker consists of six steps:\n",
    "\n",
    "1. Input Pre-Processing: Given the raw training input data, we need to pre-process it. Common input pre-processing techniques include image crop, image flip, input data normalization, and many more.\n",
    "2. Input Data Partition: Split the whole input dataset into multiple chunks and assign each chunk to one accelerator for the model training process.\n",
    "3. Data Loading: Load the data partition into the accelerators we use to train the model.\n",
    "4. Training: Train the model locally with its training input data.\n",
    "5. Model Synchronization: After generating the local gradient, synchronize it with the other worker nodes.\n",
    "6. Model Update: After getting the synchronized gradients, update the local model parameters with the aggregated gradients.\n",
    "7. Repeat Steps 4 to 6 for the successive training iterations\n",
    "    \n",
    "For the implementation, we will consider two hardware settings. First, we will use a single machine with multiple GPUs. In this setting, all the in-parallel training tasks can be launched using either a single process or multiple processes. The second type is multiple machines/nodes with multiple GPUs. In this setting, we must configure the network communication portals among all the machines. We also need to form a process group to synchronize both the cross-machine and cross-GPU training processes.\n",
    "\n",
    "#### Single-Machine/Node multi-GPU\n",
    "\n",
    "Let‚Äôs check the hardware configuration by running the command in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8471f60-8aa3-4bbd-8ec8-28727beae61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!srun --partition=gpu -n2 --gres=gpu:4 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3215be-f3e0-4dc4-ab91-e0eb58ea48e8",
   "metadata": {},
   "source": [
    "**Steps**:\n",
    "\n",
    "\n",
    "First, we need to set the default device/accelerator in the system as follows: \n",
    "\n",
    "```python\n",
    "import torch\n",
    "device = torch.device (\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "```\n",
    "\n",
    "Second, with our pre-defined model, we must pass the model to all the available devices as follows: \n",
    "\n",
    "```python\n",
    "model = torch.nn.DataParallel(model)\n",
    "```\n",
    "\n",
    "Then, PyTorch will conduct the data parallel training under the hood. When you run the whole data parallel training job, the system will launch a single process with multiple threads. Each thread is responsible for running training tasks on a single GPU. We set the default GPU to Worker 1. `nn.DataParallel()` works as follows: \n",
    "\n",
    "- We initialize the model on Worker 1 and let Worker 1 split the input training data.\n",
    "- Worker 1 will broadcast the model parameters to all the other workers (that is, Worker 2 and Worker 3). In addition, Worker 1 will also send different input data partitions to different workers (Input1 to Worker 2 and Input2 to Worker 3).\n",
    "- Then, we can start the data parallel training on all the devices. \n",
    "During each training iteration, Worker 1 needs to handle the extra operations besides its local training:\n",
    "\n",
    "<center><img src=\"images/model-synchronization.png\" width=\"350px\" height=\"350px\" alt-text=\"all-reduce\"/></center>\n",
    "<center> Model synchronization in nn.DataParallel() </center></br>\n",
    "\n",
    "After each worker generates its local gradients (for example, `Gradients0` on `Worker 1` and `Gradients1` on `Worker 2`), they send their local gradients to Worker 1. After Worker 1 aggregates all the gradients from all the workers as `Gradients_sum,` Worker 1 broadcasts `Gradients_sum` to all the other workers. \n",
    "\n",
    "When executing your program, it is also possible to specify the devices you want to use by specifying another argument, such as `device_ids.` For example, if you just want to use two GPUs, you can pass in the parameters as follows:\n",
    "```python\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1]) \n",
    "``` \n",
    "The implementation of `nn.DataParallel()` involves many `all-to-one` and `one-to-all` communications, which makes the default root node/GPU the communication bottleneck. Thus, we should adopt a scheme that evenly distributes the workloads and network communications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425bf1f-b9a3-40cb-92bc-08a7db2bf1a3",
   "metadata": {},
   "source": [
    "Let's follow the step below to run our `nn.DataParallel()` [code](../source_code/dp/main.py) using one node `(--nnodes=1)` with four GPUs `(--nproc-per-node=4)`\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- Open the terminal from the head node.\n",
    "- execute the `srun` commands to get on the computing node (the `-w dgx01` is a flag to target a specific node, and it is optional):\n",
    "  \n",
    "```text\n",
    "tade-headnode:~$ srun -p gpu -N 1 --gres=gpu:4 -w dgx01 --pty bash\n",
    "```\n",
    "- On the GPU node, run the command below to execute the `nn.DataParallel()` [code](../source_code/dp/main.py).\n",
    "\n",
    "```text\n",
    "\n",
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:4  torchrun --nnodes=1 --nproc-per-node=4  dp/main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dacc0c-f828-4fb7-ae9b-f47af96546f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:4  torchrun --nnodes=1 --nproc-per-node=4  dp/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4671c-e4b5-472b-b356-bf3f30a07177",
   "metadata": {},
   "source": [
    "**Like output on DGX A100**:\n",
    "\n",
    "```python\n",
    "W0210 12:48:11.410000 1059798 site-packages/torch/distributed/run.py:792] \n",
    "W0210 12:48:11.410000 1059798 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "W0210 12:48:11.410000 1059798 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
    "W0210 12:48:11.410000 1059798 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "Using  4 GPUs for data parallel training\n",
    "Epoch 0\n",
    "Using  4 GPUs for data parallel training\n",
    "Epoch 0\n",
    "NCCL version 2.21.5+cuda12.4\n",
    "NCCL version 2.21.5+cuda12.4\n",
    "Using  4 GPUs for data parallel training\n",
    "Epoch 0\n",
    "Using  4 GPUs for data parallel training\n",
    "Epoch 0\n",
    "...\n",
    "batch 468, loss 0.7718946933746338\n",
    "Training Done!\n",
    "batch 466, loss 1.082253336906433\n",
    "batch 464, loss 1.0795007944107056\n",
    "batch 457, loss 0.5857658982276917\n",
    "batch 467, loss 0.6964226365089417\n",
    "...\n",
    "Training Done!\n",
    "batch 466, loss 0.7101627588272095\n",
    "...\n",
    "batch 460, loss 0.8381197452545166\n",
    "batch 468, loss 0.7506725192070007\n",
    "Training Done!\n",
    "...\n",
    "batch 466, loss 0.7794694304466248\n",
    "batch 467, loss 1.0738880634307861\n",
    "batch 468, loss 0.5498190522193909\n",
    "Training Done!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dde368-8a4d-4601-b511-c069f4205882",
   "metadata": {},
   "source": [
    "#### Multi-machine/Node multi-GPU Distributed Training\n",
    "\n",
    "PyTorch distributed training on CIFAR-10 classification using DistributedDataParallel wrapped ResNet models for this section. First, we will discuss multi-process implementations for multiple machine/Node cases. Before we proceed, we need to define some concepts for multi-machine/node cases:\n",
    "\n",
    "- **rank**: A unique sequence number for all the GPUs in all the machines/node\n",
    "- **local_rank**: A sequence number for the GPUs within a machine/node\n",
    "- **world_size**: A count of all the GPUs in all the machines/nodes, which is just the total number of GPUs among all the machines/nodes\n",
    "\n",
    "For example, we have two machines, each with two GPUs. The `local_rank` for two GPUs within each machine will be `0 and 1`. Rank numbers are unique per GPU among all the machines/nodes; hence, the rank number will range from 0 to 3. Since there are four GPUs in total, `world_size` is `4`.  To implement this approach:\n",
    "\n",
    "- `nn.parallel.DistributedDataParallel()` is used instead of `nn.DataParallel()`\n",
    "  \n",
    "```python \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP \n",
    "```\n",
    "- Also import other relevant libraries for distributed data-parallel training and torch's multi-processing library as shown in the code [here](../source_code/test_ddp.py)\n",
    "- If we are not running a slurm script, we can define several system setups, such as network environments for the master node IP address and port number within the code. For a one-size-fits-all approach, it is better to get the master node IP address dynamically within the slurm script, as we don't know the exact node that will be selected.\n",
    "  \n",
    "```python\n",
    "import os def net_setup():\n",
    "os.environ['MASTER_ADDR'] = '172.31.26.15' \n",
    "os.environ['MASTER_PORT'] = '12345' \n",
    "```\n",
    "\n",
    "- Also, parse some important parameters from the user, number of epochs `(--num_epochs),` batch-size `(--batch_size),` learning rate `(--learning_rate),` etc. Other parameters, such as the number of GPUs `(--gpus),` number of machines or nodes `(--nnodes),` node rank `(--node_rank),` can be passed through the `torchrun` command.  \n",
    "\n",
    "```python\n",
    "def main(): \n",
    "    ... \n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default) \n",
    "    ...\n",
    "```\n",
    "\n",
    "Next, we need to define our distributed training function. \n",
    "-\tSet torch_seed. manual_seed is used to guarantee that we initialize the same model weights among all the worker nodes we have.\n",
    "\n",
    "```python \n",
    " def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    ...\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "```\n",
    "- Get the local rank\n",
    "\n",
    "```python\n",
    "if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        print('Local rank ', local_rank)\n",
    "```\n",
    "      \n",
    "- Initializes the distributed backend(eg NCCL or gloo) which will take care of synchronizing nodes/GPUs \n",
    "  \n",
    "```python\n",
    "\n",
    "torch.distributed.init_process_group(backend=\"nccl\") \n",
    "```\n",
    "\n",
    "- Encapsulate the model on the GPU assigned to the current process and Wrap the model with DistributedDataParallel() including each GPU's local_rank number.\n",
    "\n",
    "```python\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "```\n",
    "\n",
    "- Prefetch data (Download should be set to be False, because it is not multiprocess safe) and restrict data loading to a subset of the dataset exclusive to the current process:\n",
    "\n",
    "```python\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=False, transform=transform)\n",
    "    ...\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cbf83-8ef8-4a4e-871d-f87591d01fad",
   "metadata": {},
   "source": [
    "We can train the model with all these setups added by Loop over the dataset multiple times and save and evaluate model routinely.\n",
    "\n",
    "```python\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 10 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                ...\n",
    "\n",
    "        ddp_model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            ...\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            ...\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "With that, we have finished implementing data parallel training in multimachine/nodes and multi-GPU settings. \n",
    "\n",
    "There are two ways to run the code\n",
    "#### 1.  Run `torchrun` command as shown below on two separate nodes. \n",
    "  \n",
    "```python\n",
    " Master node:\n",
    " torchrun  --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"172.31.26.15\" --master_port=1234 test_ddp.py\n",
    "\n",
    " Worker node:\n",
    "\n",
    " torchrun  --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"172.31.26.15\" --master_port=1234 test_ddp.py\n",
    "```\n",
    "Description:\n",
    "\n",
    "```text\n",
    " --nproc_per_node: number of GPUs per node\n",
    " --nnodes: number of nodes\n",
    " --node_rank: to specified master node as rank 0 and works node as 1\n",
    " --master_addr: IP address of the master node\n",
    "```\n",
    "With this approach, you must know the master node's IP address by running the command `hostname‚Äîi` on its compute node. For example, you can quickly execute a srun command to move to the computing node (i.e. for the master node) to get the node IP Adresss. \n",
    "\n",
    "```text\n",
    " > srun -p gpu -N 1 --gres=gpu:4 --pty bash\n",
    " > hostname -i\n",
    "```\n",
    "**Note**: *partition -p might be differs from cluster to cluster. You can target a particular node by specifying `-w` within the srun command (e.g. -w dgx01)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc23a17-8346-4b9d-ada1-53f4b0afc63a",
   "metadata": {},
   "source": [
    "Let's follow the step below to run our `nn.parallel.DistributedDataParallel()` [code](../source_code/test_ddp.py) using two nodes `(--nnodes=2)` with four GPUs `(--nproc-per-node=4)`\n",
    "\n",
    "**Step 1:**\n",
    "- Open a terminal from the Jupyter Notebook.\n",
    "- execute these commands to get to the computing node:\n",
    "  \n",
    " `srun -p gpu -N 1 --gres=gpu:4  --pty bash`\n",
    " \n",
    "- Run the [master node script](../source_code/slurm/ddp_master_node.sh) using the command below. \n",
    "  \n",
    "`workspace/source_code/slurm/ddp_master_node.sh`\n",
    "\n",
    "**Likely output**:\n",
    "\n",
    "```python\n",
    "\n",
    "Master IP Address:  10.184.92.71\n",
    "W0312 06:37:46.399000 2230576 site-packages/torch/distributed/run.py:792] \n",
    "W0312 06:37:46.399000 2230576 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "W0312 06:37:46.399000 2230576 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
    "W0312 06:37:46.399000 2230576 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "Local rank  3\n",
    "Local rank  0\n",
    "Local rank  1\n",
    "Local rank  2\n",
    "...\n",
    "```\n",
    "The master node will not proceed but will wait until the worker node starts.\n",
    "- Next, copy the master IP address from the terminal (e.g. 10.184.92.71)\n",
    "- Open the worker node [script file](../source_code/slurm/ddp_worker_node.sh) and paste the master IP address (e.g. `MasterIPdress=10.184.92.71`). Then, press Ctrl + S to save the file\n",
    "- Open another terminal from the Jupyter Notebook\n",
    "- Execute this command to get to the computing node: `srun -p gpu -N 1 --gres=gpu:4 --pty bash`\n",
    " \n",
    "- Run the worker node script using this command: `workspace/source_code/slurm/ddp_worker_node.sh`\n",
    "  \n",
    "**Likely output**\n",
    "\n",
    "```python\n",
    "...\n",
    "W0210 11:54:37.849000 982813 site-packages/torch/distributed/run.py:792] \n",
    "W0210 11:54:37.849000 982813 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "W0210 11:54:37.849000 982813 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
    "W0210 11:54:37.849000 982813 site-packages/torch/distributed/run.py:792] *****************************************\n",
    "Local rank  2\n",
    "Local rank  1\n",
    "Local rank  0\n",
    "Local rank  3\n",
    "...\n",
    "NCCL version 2.21.5+cuda12.4\n",
    "Local Rank: 3, Epoch: 0, Training ...\n",
    "Local Rank: 1, Epoch: 0, Training ...\n",
    "Local Rank: 0, Epoch: 0, Training ...\n",
    "Local Rank: 2, Epoch: 0, Training ...\n",
    "---------------------------------------------------------------------------\n",
    "Epoch: 0, Accuracy: 0.0\n",
    "---------------------------------------------------------------------------\n",
    "Local Rank: 1, Epoch: 1, Training ...\n",
    "Local Rank: 3, Epoch: 1, Training ...\n",
    "Local Rank: 2, Epoch: 1, Training ...\n",
    "Local Rank: 0, Epoch: 1, Training ...\n",
    "Local Rank: 3, Epoch: 2, Training ...\n",
    "Local Rank: 0, Epoch: 2, Training ...\n",
    "...\n",
    "```\n",
    "Both master and worker nodes will proceed to execute the DDP program. Please note that you must `cancel` the job submission to be able to run the next instructions given in the notebook. \n",
    "\n",
    "- From one of the opened terminal please run the command `squeue --me`. You will see similar output as shown below.\n",
    "  ```text\n",
    "   JOBID PARTITION  NAME       USER  ST       TIME    TIME_LEFT  CPUS MIN_MEM  NODE NODELIST(REASON)\n",
    "  15648   gpu       bash       tade  R      1:32:46   10:27:14     2     0      1     dgx02\n",
    "  15647   gpu       bash       tade  R      1:36:31   10:23:29     2     0      1     dgx01\n",
    "  ```\n",
    "- To cancel each of the job, use the command: `scancel <JOBID>.` For example, `scancel 15648` and `scancel 15647`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c75a1d-aa0f-490b-8fe1-30109bd93a5c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25490fd0-14b6-4ae5-9fab-b1615674c6d2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 2.  Run `torchrun` with Slurm Command\n",
    "\n",
    "A major disadvantage of the above approach is that if you intend to run the program on five or more nodes, You will be required to open five or more terminals and execute the process simultaneously, which takes a lot of effort. The Slurm approach is a way to simplify the process by having just a single script containing slurm commands that provide the opportunity to run your application on many nodes available from one point. Let's dive deep into a sample slurm script for our `nn.parallel.DistributedDataParallel()` code.\n",
    "\n",
    "**Sbatch section:** This is the section to set the number of nodes, the GPU partition, the number of tasks per node, the number of threads to be executed per task, the number of GPU to use per node, the name given to the submitted job, name of the file to output errors and warnings, and name of a file to output training progress and results.\n",
    "\n",
    "```text\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=2               # number of nodes\n",
    "#SBATCH --partition=gpu         # GPU partition\n",
    "#SBATCH --ntasks-per-node=4     # number of tasks per node\n",
    "#SBATCH --cpus-per-task=8       # number of threads per task\n",
    "#SBATCH --threads-per-core=1    # number of threads per core\n",
    "#SBATCH --gres=gpu:4            # number of gpus per node\n",
    "#SBATCH --time=12:00:00         # format: HH:MM:SS\n",
    "...\n",
    "#SBATCH --job-name=ddpslurm     # job name\n",
    "#SBATCH -o %x.output%j          # name of the file to output errors and warnings\n",
    "#SBATCH -e %x.error%j           # name of a file to output training progress and results\n",
    "```\n",
    "\n",
    "**Libraries/virtual environment settings**: This section might differ from cluster to cluster. The dependencies required by your application determine it. In the example below, we load a conda library and activate a conda environment that contains all the dependencies needed to run our `nn.parallel.DistributedDataParallel()` code.\n",
    "\n",
    "```text\n",
    "\n",
    "# load conda env\n",
    "\n",
    "module load conda/2023\n",
    "conda init\n",
    "source activate env_mnode\n",
    "```\n",
    "\n",
    "**Getting master node IP address**: This is the part of the slurm script that dynamically gets the master node IP address (head_node_ip). \n",
    "\n",
    "```text\n",
    "# get master node IP Address \n",
    "...\n",
    "nodes=( $(scontrol show hostnames \"$SLURM_JOB_NODELIST\"))\n",
    "nodes_array=($nodes)\n",
    "head_node=${nodes_array[0]}\n",
    "head_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname --ip-address)\n",
    "```\n",
    "\n",
    "**Executing Srun command**: This is the section that starts two nodes, and submits using a simple command below:\n",
    "\n",
    "`srun python source_code/slurm_ddp.py`\n",
    "\n",
    "*For reference purpose, you can also submit a job using `torchrun` command. Within the torchrun command you set two [Rendezvous](https://pytorch.org/docs/stable/elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler) flags `--rdzv_backend` that uses a `C10d` store (by default TCPStore) as the rendezvous backend, and `--rdzv_endpoint` that takes ip address and port number as input. You can read more on  [Rendezvous flags here](https://pytorch.org/docs/stable/elastic/run.html). For example: `srun torchrun --nproc_per_node=4 --nnodes=2 --rdzv_backend c10d --rdzv_endpoint $head_node_ip:29500 source_code/test_ddp.py`*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717649a-72b0-4e75-a390-27066bad774c",
   "metadata": {},
   "source": [
    "Let's follow the steps below to run the slurm script [found here](../source_code/slurm/ddp_multinode.slurm):\n",
    "\n",
    "- Open a terminal from the head node and navigate to the project workspace folder.\n",
    "\n",
    "- Use `sbatch` command to execute the Slurm script: `sbatch workspace/source_code/slurm/ddp_multinode.slurm`\n",
    "\n",
    "- Check if your job was submitted successfully using the `squeue --me` command. This will show the list of jobs you submitted.\n",
    "\n",
    "````text\n",
    "\n",
    " JOBID PARTITION   NAME       USER ST         TIME    TIME_LEFT  CPUS MIN_MEM  NODE NODELIST(REASON)\n",
    " 14695  gpu      ddpslurm      tade  R         0:05     11:59:55   512    0      2   dgx[01-02]\n",
    "````\n",
    "- On successful execution, you can check for the output progress (`ddpslurm.outputxxx`) in the workspace directory. In case you find on ouput there, please check for warnings and errors in the error file `ddpslurm.errorxxx.`\n",
    "\n",
    "Likely output from the `ddpslurm.outputxxx`(<NAME.outputJOBID>).\n",
    "\n",
    "```python\n",
    "...\n",
    "Local rank  1\n",
    "Local rank  0\n",
    "Local rank  1\n",
    "Local rank  2\n",
    "Local rank  0\n",
    "Local rank  3\n",
    "NCCL version 2.21.5+cuda12.4\n",
    "Local rank  2\n",
    "Local rank  3\n",
    "Local Rank: 3, Epoch: 0, Training ...\n",
    "Local Rank: 0, Epoch: 0, Training ...\n",
    "Local Rank: 2, Epoch: 0, Training ...\n",
    "Local Rank: 1, Epoch: 0, Training ...\n",
    "Local Rank: 1, Epoch: 0, Training ...\n",
    "Local Rank: 2, Epoch: 0, Training ...\n",
    "Local Rank: 3, Epoch: 0, Training ...\n",
    "Local Rank: 0, Epoch: 0, Training ...\n",
    "---------------------------------------------------------------------------\n",
    "Epoch: 0, Accuracy: 0.0\n",
    "---------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------\n",
    "Epoch: 0, Accuracy: 0.0\n",
    "---------------------------------------------------------------------------\n",
    "Local Rank: 3, Epoch: 1, Training ...\n",
    "Local Rank: 2, Epoch: 1, Training ...\n",
    "Local Rank: 2, Epoch: 1, Training ...\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87bb3b-4ed5-49cd-b263-e7d15615ea66",
   "metadata": {},
   "source": [
    "### Model Checkpointing and Fault Tolerance\n",
    "\n",
    "In distributed training, a single process failure can disrupt the entire training job. Since the training process failure rate can be higher, making your training script robust is particularly important. PyTorch offers a utility called `torchrun` that provides [fault-tolerance](https://pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html) and [elastic training](https://pytorch.org/docs/stable/elastic/run.html). When a failure occurs, torchrun logs the errors and attempts to automatically restart all the processes from the last saved ‚Äúsnapshot‚Äù of the training job. The snapshot saves more than just the model state; it can include details about the number of epochs run, optimizer states, or any other stateful attribute of the training job necessary for its continuity. Model checkpointing is all about achieving in-parallel model saving. Below is a sample custom checkpointing snapshot function\n",
    "\n",
    "```python\n",
    "\n",
    "def checkpointing(rank, epoch, net, optimizer, loss):\n",
    "\tpath = f\"model{rank}.pt\"\n",
    "\ttorch.save({\n",
    "\t\t\t\t'epoch':epoch,\n",
    "\t\t\t\t'model_state':net.state_dict(),\n",
    "\t\t\t\t'loss': loss,\n",
    "\t\t\t\t'optim_state': optimizer.state_dict(),\n",
    "\t\t\t\t}, path)\n",
    "\tprint(f\"Checkpointing model {rank} done.\")\n",
    "    \n",
    "```\n",
    "\n",
    "Let's proceed to the `Fully Sharded Data Parallelism (FSDP)` notebook. Here, you will learn a concept that improves on data parallelism. Please click the [Next Link](fsdp.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b41c1-c915-4fd0-8f5b-b70e48dd3a96",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright ¬© 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1a178-5555-4134-9d42-a6bc15cd01cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
