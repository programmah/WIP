{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52d54dc-93c9-4256-8c5b-84aa57b3959b",
   "metadata": {},
   "source": [
    "# Tensor Parallelism\n",
    "---\n",
    "\n",
    "This notebook focuses on the need for tensor parallel and explains key concepts that include `Column-wise parallel,` `Row-wise parallel,` and `Combined Column-wise and Row-wise Parallel.` It further gave an overview of `Device Mesh` and demonstrated how to implement Tensor/Sequence parallel with Fully Sharded Data Parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e302a4-6030-4cfc-9568-d7e2dfceb40f",
   "metadata": {},
   "source": [
    "Tensor parallelism (TP) is a training technique used for large models. TP distributes layers across multiple devices and reduces inter-device communication to improve memory management and efficiency. Tensor Parallelism splits tensor into N chunks along a particular dimension such that each device only holds 1/N chunk of the tensor. Computation is performed using this partial chunk to get partial output. These partial outputs are collected from all devices ensuring the correctness of the computation is maintained. The technique was initially proposed in the [Megatron-LM paper](https://arxiv.org/abs/1909.08053) as an efficient model parallelism approach to train large-scale Transformer models. However, when the model becomes larger, the activation memory becomes the bottleneck. Therefore, TP applies a Sequence Parallel (SP) strategy (a parallel strategy that partitions along the sequence dimension) to the LayerNorm or RMSNorm layers.\n",
    "\n",
    "\n",
    "#### Why Use Tensor Parallel\n",
    "\n",
    "With Pytorch, Fully Sharded Data Parallel (FSDP) has a scaling limit on the number of GPUs used in training a model. Further scaling attempts result in challenges that require solving by combining Tensor Parallel with FSDP. For example:\n",
    "\n",
    "- As the number of GPUs becomes large (exceeding 128/256 GPUs), the FSDP all-gather operation gets dominated by ring latency. Combining FSDP and TP in a way that only the FSDP is inter-host could reduce the world size by 8 and decrease the latency costs.\n",
    "- When data parallelism is faced with both convergence and GPU memory limitations, applying TP/SP could ballpark the global batch size and enable scaling with more GPUs\n",
    "- When the local batch size becomes smaller for some models, TP/SP can yield matrix multiplication shapes that are more optimized for floating point operations (FLOPS)\n",
    "\n",
    "\n",
    "In tensor parallelism, the computation of a linear layer can be split up across GPUs. This saves memory because each GPU only needs to hold a portion of the weight matrix. Pytorch has a set of Parallel styles (ParallelStyle) to configure sharding for Model layers. This includes `ColwiseParallel,` `RowwiseParallel,` `SequenceParallel,` `PrepareModuleInput,` and `PrepareModuleOutput.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ef948-7153-431d-9bab-b5c7fca58c2a",
   "metadata": {},
   "source": [
    "### Column-wise Parallel\n",
    "\n",
    "In a column-wise parallel layer, the weight matrix is split evenly along the column dimension. Each GPU is sent the same input and computes a regular matrix multiplication with its portion of the weight matrix. At the end, the outputs from each GPU can be concatenated to form the final output. For example, let `A = XB`. We can split B along the column dimension into `(B0 B1 B2 … Bn)`. Each device holds a column for matrix multiplication and possesses partial results( e.g., device rank 0 holds XB0). To ensure a correct result, the `all-gather` operation is performed on the partial result to concatenate the tensor along the column dimension.\n",
    "\n",
    "<center><img src=\"images/columnwise.png\" width=\"550px\" height=\"550px\" alt-text=\"fsdp workflow\"/></center>\n",
    "<center> <a href=\"https://lightning.ai/docs/pytorch/stable/_images/tp-colwise.jpeg\">image source</a></center>\n",
    "\n",
    "### Row-wise Parallel\n",
    "\n",
    "This form of parallelism evenly divides weight matric across available devices. Because the weight matrix now has fewer rows, it uses the same approach to split the input along its dimension(column), as shown in the screenshot below. Each GPU then performs matrix multiplication using its portion of the weight matrix and inputs. All-reduce operation is performed on the outputs from each GPU to form the final output. For example, let `A = XB`. We can split B along its row into `(B0 B1 B2 … Bn)`. Each device holds a row for matrix multiplication with the input, resulting in a partial result (e.g., device rank 0 holds X0B0). To ensure a correct result, an `all-reduce` sum is performed on the partial results to produce the final output.\n",
    "\n",
    "<center><img src=\"images/rowwise.png\" width=\"550px\" height=\"550px\" alt-text=\"row\"/></center>\n",
    "<center><a href=\"https://lightning.ai/docs/pytorch/stable/_images/tp-rowwise.jpeg\">image source</a></center>\n",
    "\n",
    "### Combined Column-wise and Row-wise Parallel\n",
    "\n",
    "The column-wise and row-wise parallel styles can be combined for maximum effect on multiple linear layers in sequence such as we have in an MLP or a Transformer. The output of the column-wise parallel layer is kept separate and is fed directly to the row-wise parallel layer to avoid costly data transfers between GPUs. A typical illustration of using Columnwise Parallel and Row-wise Parallel linear layers can be found in the Megatron-LM paper, titled, *[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)*.\n",
    "\n",
    "<center><img src=\"images/rowcolumnwise.png\" width=\"550px\" height=\"550px\" alt-text=\"row\"/></center>\n",
    "<center><a href=\"https://lightning.ai/docs/pytorch/stable/_images/tp-combined.jpeg\">image source</a></center>\n",
    "<i>Note that activation functions between the layers can still be applied without additional communication because they are element-wise, but are not shown in the screenshot for simplicity. </i> <br/>\n",
    "\n",
    "\n",
    "Tensor Parallel shard individual tensors over a set of devices in a distributed environment (such as NCCL communicators or Gloo). Tensor Parallelism is a `Single-Program Multiple-Data (SPMD)` sharding algorithm that leverages the PyTorch `DTensor` to perform `sharding.` It also utilizes the `DeviceMesh` abstraction (manages ProcessGroups under the hood) for device management and sharding. Without `DeviceMesh,` users would need to manually set up NCCL communicators and CUDA devices on each process before applying any parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd9ae5-e12e-44a0-b88c-856673ad14e3",
   "metadata": {},
   "source": [
    "### Brief Overview of Device Mesh\n",
    "\n",
    "[DeviceMesh](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html) is a higher-level abstraction that manages `ProcessGroup.` It allows users to effortlessly create inter-node and intra-node process groups without worrying about how to set up ranks correctly for different sub-process groups. Users can also easily manage the underlying process_groups/devices for multi-dimensional parallelism via `DeviceMesh.` The screenshot below shows that a 2D mesh can be created to connect devices within each host and connect each device with its counterpart on the other hosts in a homogenous setup.\n",
    "\n",
    "<center><img src=\"images/device_mesh.png\" width=\"550px\" height=\"550px\" alt-text=\"device-mesh\"/></center>\n",
    "<center><a href=\"https://pytorch.org/tutorials/_images/device_mesh.png\">image source</a></center>\n",
    "\n",
    "Tensor Parallel usually works within each host. We can initialize a DeviceMesh that connects 8 GPUs within a host using `init_device_mesh()` as follows:\n",
    "\n",
    "```python\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "\n",
    "tp_mesh = init_device_mesh(\"cuda\", (8,))\n",
    "\n",
    "```\n",
    "\n",
    "A 2D setup and access to the underlying `ProcessGroup` can be created using a sample code is given below.\n",
    "\n",
    "```python\n",
    "\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "mesh_2d = init_device_mesh(\"cuda\", (2, 4), mesh_dim_names=(\"replicate\", \"shard\"))\n",
    "\n",
    "# Users can access the underlying process group through the `get_group` API.\n",
    "replicate_group = mesh_2d.get_group(mesh_dim=\"replicate\")\n",
    "shard_group = mesh_2d.get_group(mesh_dim=\"shard\")\n",
    "\n",
    "```\n",
    "Checkout for detailed documentation on DeviceMesh and its use for custom parallel solutions [here](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html#getting-started-with-devicemesh).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e248f-b618-4979-b0db-e80740050efc",
   "metadata": {},
   "source": [
    "### Tensor Parallel Implementation with An MLP Model\n",
    "\n",
    "This section tests Tensor Parallel(TP) implementation with a toy MLP model in a Megetron-LM Single-Program Multiple-Data (SPMD) style. We show an end-to-end working flow from forward, backward, and optimization. The sample code includes two `nn.Linear` layers with an element-wise `nn.RELU.` The basic idea is that the first linear layer is parallelized column-wise while the second linear layer is parallelized row-wise, so only one `all-reduce` is done at the end of the second linear layer. This way, communications between two layers are avoided, and the model training is sped up.\n",
    "\n",
    "**Steps**\n",
    "- Define the Model layer class\n",
    "\n",
    "```python\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP based model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.in_proj = nn.Linear(10, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_proj = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out_proj(self.relu(self.in_proj(x)))\n",
    "```\n",
    "\n",
    "- Create a device mesh based on the given world_size\n",
    "  \n",
    "```python\n",
    "_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "device_mesh = init_device_mesh(device_type=\"cuda\", mesh_shape=(_world_size,))\n",
    "_rank = device_mesh.get_rank()\n",
    "```\n",
    "-  Create a custom parallelization plan for the model\n",
    "\n",
    "```python\n",
    "...\n",
    "tp_model = parallelize_module(\n",
    "    module=tp_model,\n",
    "    device_mesh=device_mesh,\n",
    "    parallelize_plan={\n",
    "        \"in_proj\": ColwiseParallel(),\n",
    "        \"out_proj\": RowwiseParallel(),\n",
    "    },\n",
    ")\n",
    "...\n",
    "```\n",
    "Please find the complete code [here](../source_code/tensor_parallel_example.py). You can run the Tensor parallelism sample code using 4 GPUs (within a node) in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788152f-dc25-48b4-9249-1744a1ae43b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:4 torchrun --nnodes=1 --nproc_per_node=4 --rdzv_id=101 --rdzv_endpoint=\"localhost:5972\" tensor_parallel_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123204a-c839-4949-856a-0805dbfc6205",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "Starting PyTorch TP example on rank 0.\n",
    "02/22/2025 07:49:00 AM  Device Mesh created: device_mesh=DeviceMesh('cuda', [0, 1, 2, 3])\n",
    "Starting PyTorch TP example on rank 2.\n",
    "Starting PyTorch TP example on rank 3.\n",
    "Starting PyTorch TP example on rank 1.\n",
    "02/22/2025 07:49:09 AM  Tensor Parallel training starting...\n",
    "02/22/2025 07:49:09 AM  Tensor Parallel iter 0 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 1 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 2 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 3 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 4 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 5 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 6 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 7 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 8 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel iter 9 completed\n",
    "02/22/2025 07:49:10 AM  Tensor Parallel training completed!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c5502-2961-48b2-8288-4f387655e22a",
   "metadata": {},
   "source": [
    "### Tensor/Sequence parallel with Fully Sharded Data Parallel (TP/SP + FSDP)\n",
    "\n",
    "This section describes the implementation of 2D Parallel, which combines Tensor/Sequence parallel with Fully Sharded Data Parallel (TP/SP + FSDP) using a [Llama2 model]. It further describes the working flow of the forward and backward passes. In the implementation, Fully Sharded Data Parallel + Tensor Parallel are enabled in separate parallel dimensions: `Data Parallel (\"dp\") across hosts` and `Tensor Parallel (\"tp\") within each host.`\n",
    "\n",
    "**Illustration steps:**\n",
    "\n",
    "- Initialize the world size and rank topology\n",
    "```python\n",
    "...\n",
    "_rank = int(os.environ[\"RANK\"])\n",
    "_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "...\n",
    "```\n",
    "- Create a sharding plan based on the given world_size\n",
    "```python\n",
    "dp_size = _world_size // tp_size\n",
    "```\n",
    "- Create a device mesh with 2 dimensions (data parallel dimension, tensor parallel dimension).\n",
    "\n",
    "```python\n",
    "...\n",
    "device_mesh = init_device_mesh(\"cuda\", (dp_size, tp_size), mesh_dim_names=(\"dp\", \"tp\"))\n",
    "rank_log(_rank, logger, f\"Device Mesh created: {device_mesh=}\")\n",
    "tp_mesh = device_mesh[\"tp\"]\n",
    "dp_mesh = device_mesh[\"dp\"]\n",
    "\n",
    "...\n",
    "```\n",
    "- For the Tensors Parallel, input must be the same across all TP ranks, while for SP, input can be different across all ranks. Let's use dp_rank to set the random seed and mimic the behavior of the dataloader.\n",
    "\n",
    "```python\n",
    "dp_rank = dp_mesh.get_local_rank()\n",
    "\n",
    "```\n",
    "- Instantiate the llama model and move it to GPU.\n",
    "\n",
    "```python \n",
    "simple_llama2_config = ModelArgs(dim=256, n_layers=2, n_heads=16, vocab_size=32000)\n",
    "\n",
    "model = Transformer.from_model_args(simple_llama2_config).to(\"cuda\")\n",
    "...\n",
    "```\n",
    "\n",
    "- For each transformer block in the llama model, apply sequence parallel (SequenceParallel) to the `normalization_norm` and `ffn_norm` layers. To the feed_forward (w1,w3) and attention (wq,wk,wv), apply column-wise parallel, while to both (same for w2 and w3).\n",
    "\n",
    "```python\n",
    "\n",
    "for layer_id, transformer_block in enumerate(model.layers):\n",
    "    layer_tp_plan = {\n",
    "        \"attention_norm\": SequenceParallel(),\n",
    "        \"attention\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1), None),\n",
    "            desired_input_layouts=(Replicate(), None),\n",
    "        ),\n",
    "        \"attention.wq\": ColwiseParallel(),\n",
    "         ...\n",
    "\n",
    "        \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"ffn_norm\": SequenceParallel(),\n",
    "        \"feed_forward\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1),),\n",
    "            desired_input_layouts=(Replicate(),),\n",
    "        ),\n",
    "        \"feed_forward.w1\": ColwiseParallel(),\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "- Initialize a custom parallelization plan for the model\n",
    "  \n",
    "```python\n",
    "    parallelize_module(\n",
    "        module=transformer_block,\n",
    "        device_mesh=tp_mesh,\n",
    "        parallelize_plan=layer_tp_plan\n",
    "    )\n",
    "```\n",
    "- Initialize FSDP using the dp device mesh\n",
    "\n",
    "```python\n",
    "sharded_model = FSDP(model, device_mesh=dp_mesh, use_orig_params=True)\n",
    "```\n",
    "- Run a training loop to perform iterations for the forward and backward passes\n",
    "\n",
    "```python\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    ...\n",
    "    output = sharded_model(inp)\n",
    "    output.sum().backward()\n",
    "    optimizer.step()\n",
    "    ...\n",
    "```\n",
    "\n",
    "Please find the complete code [here](../source_code/fsdp_tp_example.py). Let's execute the TP/SP + FSDP sample code with 4 GPUs (in a node) by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc3855-c8a0-4d6b-b8f6-35d4ae04e59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:4 torchrun --nnodes=1 --nproc_per_node=4 --rdzv_id=101 --rdzv_endpoint=\"localhost:5972\" fsdp_tp_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfb8f1-355b-4eae-849c-ea2d10d9dbac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Likely Output using 4 GPUs:**\n",
    "\n",
    "```python\n",
    "...\n",
    "Starting PyTorch 2D (FSDP + TP) example on rank 0.\n",
    "Starting PyTorch 2D (FSDP + TP) example on rank 3.Starting PyTorch 2D (FSDP + TP) example on rank 2.\n",
    "\n",
    "Starting PyTorch 2D (FSDP + TP) example on rank 1.\n",
    "02/22/2025 09:45:20 AM  Device Mesh created: device_mesh=DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('dp', 'tp'))\n",
    "02/22/2025 09:45:26 AM  Model after parallelization sharded_model=FullyShardedDataParallel(\n",
    "  (_fsdp_wrapped_module): Transformer(\n",
    "    (tok_embeddings): Embedding(32000, 256)\n",
    "    (layers): ModuleList(\n",
    "      (0-1): 2 x TransformerBlock(\n",
    "        (attention): Attention(\n",
    "          (wq): Linear(in_features=256, out_features=256, bias=False)\n",
    "          (wk): Linear(in_features=256, out_features=256, bias=False)\n",
    "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
    "          (wo): Linear(in_features=256, out_features=256, bias=False)\n",
    "        )\n",
    "        (feed_forward): FeedForward(\n",
    "          (w1): Linear(in_features=256, out_features=768, bias=False)\n",
    "          (w2): Linear(in_features=768, out_features=256, bias=False)\n",
    "          (w3): Linear(in_features=256, out_features=768, bias=False)\n",
    "        )\n",
    " ...     \n",
    "\n",
    "02/22/2025 09:45:28 AM  2D iter 7 complete\n",
    "02/22/2025 09:45:28 AM  2D iter 8 complete\n",
    "02/22/2025 09:45:28 AM  2D iter 9 complete\n",
    "02/22/2025 09:45:28 AM  2D training successfully completed!\n",
    "\n",
    "```\n",
    "The key concepts of this notebook were adopted from the [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp.html) documentation page and the codes from [Pytorch GitHub](https://github.com/pytorch/examples/tree/main/distributed/tensor_parallelism). Let's proceed to the next notebook and learn about `Message Passing and Mixed Precision.` Please click the [Next Link](other-topics.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2dfb84-bdf4-4651-aa31-031cddf5205b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html\n",
    "- https://pytorch.org/tutorials/intermediate/TP_tutorial.html\n",
    "- https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp.html\n",
    "- https://github.com/pytorch/examples/tree/main/distributed/tensor_parallelism\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
