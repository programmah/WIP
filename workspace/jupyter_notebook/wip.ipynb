{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da04a4fc-5fc7-40e0-a025-bf7b461f9e1e",
   "metadata": {},
   "source": [
    "# This notebook is a work in progress\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Horovod\n",
    "\n",
    "[Horovod](https://github.com/horovod/horovod) is a distributed deep-learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Its goal is to make distributed deep learning fast and easy to use. Horovod is an open-source tool initially developed by Uber to support their need for faster deep-learning model training across many engineering teams. It is part of a growing ecosystem of approaches to distributed training, including, for example, Distributed PyTorch. Uber developed a solution that utilized MPI for distributed process communication and the [NVIDIA Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl) for its highly optimized implementation of reductions across distributed processes and nodes. The resulting Horovod package delivers on its promise to scale deep learning model training across multiple GPUs and multiple nodes with only minor code modification and intuitive debugging.\n",
    "\n",
    "To use [Horovod with PyTorch](https://horovod.readthedocs.io/en/latest/pytorch.html), make the following modifications to your training script:\n",
    "\n",
    "1. Run `hvd.init().`\n",
    "\n",
    "2. Pin each GPU to a single process. With the typical setup of one GPU per process, set this to local rank.\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "```\n",
    "3. Scale the learning rate by the number of workers. This is because the number of workers effectively scales batch size in synchronous distributed training. Note that increasing the learning rate compensates for the increased batch size.\n",
    "\n",
    "4. Wrap the optimizer in `hvd.DistributedOptimizer.` The function of the distributed optimizer is to delegate gradient computation to the original optimizer, averages gradients using `allreduce` or `allgather,` and then applies those averaged gradients.*\n",
    "\n",
    "5. Broadcast the initial variable states from rank 0 to all other processes:\n",
    "\n",
    "```python\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "```\n",
    "This step is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\n",
    "\n",
    "6. Modify your code to save checkpoints only on `worker 0` to prevent other workers from corrupting them. You can do this by conditioning the model checkpointing code with `hvd.rank() != 0.`\n",
    "\n",
    "A sample code block demonstrating how to apply the above-itemized steps is given below\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "import horovod.torch as hvd\n",
    "\n",
    "# Initialize Horovod\n",
    "hvd.init()\n",
    "\n",
    "# Pin GPU to be used to process local rank (one GPU per process)\n",
    "torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "# Define dataset...\n",
    "train_dataset = ...\n",
    "\n",
    "# Partition dataset among workers using DistributedSampler\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)\n",
    "\n",
    "# Build model...\n",
    "model = ...\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters())\n",
    "\n",
    "# Add Horovod Distributed Optimizer\n",
    "optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "\n",
    "# Broadcast parameters from rank 0 to all other processes.\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "for epoch in range(100):\n",
    "   for batch_idx, (data, target) in enumerate(train_loader):\n",
    "       optimizer.zero_grad()\n",
    "       output = model(data)\n",
    "       loss = F.nll_loss(output, target)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       if batch_idx % args.log_interval == 0:\n",
    "           print('Train Epoch: {} [{}/{}]\\tLoss: {}'.format(\n",
    "               epoch, batch_idx * len(data), len(train_sampler), loss.item()))\n",
    "\n",
    "```\n",
    "\n",
    "To run a sample training code with 2 GPUs on a single machine, use the command:\n",
    "\n",
    "```text\n",
    "horovodrun -np 2 python3 training.py \n",
    "\n",
    "or use: \n",
    "\n",
    "horovodrun -np 2 -H localhost:2 python3 training.py  \n",
    "```\n",
    "To run a sample training code with 16 GPUs on four machines (4 GPUs each), use the command:\n",
    "\n",
    "```text\n",
    "horovodrun -np 16 -H hostname_1:4,hostname_2:4,hostname_3:4,hostname_4:4 python3 training.py\n",
    "```\n",
    "*`-np` denotes the number of GPUs, and `-H` represents the hostname or server name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bb520-2b33-4c92-abda-1024120ea5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!horovodrun -np 2 --mpi-args=\"--oversubscribe\"  python3 ../source_code/horovod_pytorch_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebea441-a34c-4b1b-86e6-56f73b490783",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- https://horovod.readthedocs.io/en/latest/pytorch.html\n",
    "- https://github.com/horovod/horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a896fe9-ee68-4026-9309-66a7ad3e497f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
