{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437144db-25d2-4b81-a16f-e03179eefe15",
   "metadata": {},
   "source": [
    "# Fully Sharded Data Parallel(FSDP)\n",
    "---\n",
    "\n",
    "The goal of this notebook is to walk you through the implementation of the training strategy called Fully Sharded Data Parallel(FSDP) and how it improves data parallelism. This content is adapted from [Pytorch webpage](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) for learning purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f9a3b-26f7-41ee-b179-656484b61278",
   "metadata": {},
   "source": [
    "In distributed data parallelism (DDP) training, each worker or device has a replica of the model parameters and optimizer to train a batch of data. After the training, the backend (`NCCL or Gloo`) applies an all-reduce operation, to sum up gradients over different workers or devices and update them.  [Fully sharded Data Parallel (FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) is a type of data parallelism that shards model parameters, optimizer states, and gradients across DDP ranks. During training, the FSDP GPU memory footprint is smaller than DDP, making training very large models possible but at the cost of communication overheads. However, through overlapping communication techniques, the overhead can be significantly minimized.\n",
    "\n",
    "#### FSDP Workflow\n",
    "\n",
    "FSDP includes three major parts, namely `In constructor,` `In forward path,` and `In backward path.` \n",
    "\n",
    "- **In constructor**: Shard model parameters and each rank or device keeps only its shard\n",
    "- **In forward path**: This unit recovers the full parameters by executing the all-gather operation to collect shards from ranks or devices. It runs the forward pass and discards the parameters of the shards it collected.\n",
    "- **In backward path**: Performs a task similar to the `In forward path` except for `forward pass` but executes the backward computation and `reduce-scatter` operation to sync gradients.\n",
    "\n",
    "<center><img src=\"images/fsdp-arc.png\" width=\"550px\" height=\"550px\" alt-text=\"fsdp workflow\"/></center>\n",
    "<center> FSDP Workflow <a href=\"https://pytorch.org/tutorials/_images/fsdp_workflow.png\" >[view image source]</a> </center>\n",
    "\n",
    "#### FSDP Sharding Process\n",
    "\n",
    "To fully understand how the FSDP sharding process works, it is crucial to decompose the `all-reduce` operation in DDP into `reduce-scatter` and `all-gather` operations as shown in the screenshot below. During the backward pass, each rank or device possesses a shard of the gradients through the reduce-scatter operation. It proceeds to update the corresponding shard of the parameters in the optimizer step. FSDP executes an `all-gather` operation in the next forward pass to gather and combine the updated parameter shards.\n",
    "\n",
    "<center><img src=\"images/fsdp-allreduce.png\" width=\"550px\" height=\"550px\" alt-text=\"fsdp workflow\"/></center>\n",
    "<center> FSDP All Reduce <a href=\"https://pytorch.org/tutorials/_images/fsdp_sharding.png\" >[view image source]</a> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd9357-2769-4acf-8db3-86997e59159b",
   "metadata": {},
   "source": [
    "#### FSDP Implementation \n",
    "\n",
    "Let's use a toy model to demonstrate the process using the MNIST dataset. You can also apply the steps highlighted below to train larger models.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "- Initialize the group process distributed training and clean-up functions.\n",
    "\n",
    "```python\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "```\n",
    "- Model definition. For example, we define a toy model for handwritten digit classification.\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        ...\n",
    "        return output\n",
    "```\n",
    "- Define train and test functions, calculate the loss, and perform all-reduce opration within them\n",
    "\n",
    "```python\n",
    "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n",
    "    model.train()\n",
    "    ddp_loss = torch.zeros(2).to(rank)\n",
    "    ...\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        ...\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ddp_loss[0] += loss.item()\n",
    "        ddp_loss[1] += len(data)\n",
    "\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "    ...\n",
    "\n",
    "def test(model, rank, world_size, test_loader):\n",
    "   ...\n",
    "        for data, target in test_loader:\n",
    "            ...\n",
    "            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n",
    "            ddp_loss[2] += len(data)\n",
    "\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    if rank == 0:\n",
    "        test_loss = ddp_loss[0] / ddp_loss[2]\n",
    "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n",
    "            100. * ddp_loss[1] / ddp_loss[2]))\n",
    "```\n",
    "- The last function would be a train function that wraps the model in FSDP.\n",
    "\n",
    "```python\n",
    "\n",
    "def fsdp_main(rank, world_size, args):\n",
    "    setup(rank, world_size)\n",
    "    ...\n",
    "    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n",
    "    ...\n",
    "    \n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        size_based_auto_wrap_policy, min_num_params=100\n",
    "    )\n",
    "    torch.cuda.set_device(rank)\n",
    "    ...\n",
    "    model = Net().to(rank)\n",
    "    model = FSDP(model)\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    init_start_event.record()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n",
    "        test(model, rank, world_size, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    ...\n",
    "\n",
    "    cleanup()\n",
    "```\n",
    "\n",
    "You can open the complete FSDP code [from here](../source_code/test_fsdp.py). Please run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff412e-199d-4b3b-8285-0138ec010520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:4 python test_fsdp.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480949ef-2721-4f7c-96a2-2f1c2ed4a9b1",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "Train Epoch: 10 \tLoss: 0.026925\n",
    "Test set: Average loss: 0.0272, Accuracy: 9916/10000 (99.16%)\n",
    "\n",
    "CUDA event elapsed time: 80.868640625sec\n",
    "FullyShardedDataParallel(\n",
    "  (_fsdp_wrapped_module): Net(\n",
    "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (dropout1): Dropout(p=0.25, inplace=False)\n",
    "    (dropout2): Dropout(p=0.5, inplace=False)\n",
    "    (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
    "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "\n",
    "```\n",
    "The output clearly shows that the model is wrapped in one FSDP unit. Consequently, it reduces computation efficiency and memory efficiency because there is only one blocking `all-gather` call for all 100 linear layers; hence, communication and computation overlap between layers is lacking. To remedy the situation, we increase `min_num_params` for the based size of auto wrap policy, define the `auto_wrap_policy,` and pass it to the FSDP wrapper as shown below.\n",
    "\n",
    "```python\n",
    "my_auto_wrap_policy = functools.partial(size_based_auto_wrap_policy, min_num_params=20000 )\n",
    "torch.cuda.set_device(rank)\n",
    "model = Net().to(rank)\n",
    "model = FSDP(model, auto_wrap_policy=my_auto_wrap_policy)\n",
    "```\n",
    "\n",
    "Todo: In the [FSDP code](../source_code/test_fsdp.py), comment link lines `#134 and #144`. Next, uncomment lines `#136 and #145`. Please run the cell below to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59ffa1-4425-44fb-9155-c45979346489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && srun -p gpu -N 1 --gres=gpu:4 python test_fsdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb07a6-c955-41a6-9573-43c104aabf5f",
   "metadata": {},
   "source": [
    "**Likely output**:\n",
    "```python\n",
    "...\n",
    "Train Epoch: 10 \tLoss: 0.023942\n",
    "Test set: Average loss: 0.0255, Accuracy: 9922/10000 (99.22%)\n",
    "\n",
    "CUDA event elapsed time: 79.628578125sec\n",
    "FullyShardedDataParallel(\n",
    "  (_fsdp_wrapped_module): Net(\n",
    "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (dropout1): Dropout(p=0.25, inplace=False)\n",
    "    (dropout2): Dropout(p=0.5, inplace=False)\n",
    "    (fc1): FullyShardedDataParallel(\n",
    "      (_fsdp_wrapped_module): Linear(in_features=9216, out_features=128, bias=True)\n",
    "    )\n",
    "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "```\n",
    "Now, we have the model wrapped in two FSDP units. Profiling this code, you will find out that the FSDP Peak Memory usage has been reduced using the Auto_wrap policy. `CPU offloading` can be used if you have a very large model that won't fit into GPUs with FSDP. Currently, PyTorch only supports parameter and gradient CPU offload. It can be enabled by specifying this line within the FSDP wrapper: `cpu_offload= CPUOffload(offload_params=True).`\n",
    "\n",
    "```python\n",
    "model = FSDP(model, auto_wrap_policy=my_auto_wrap_policy, cpu_offload=CPUOffload(offload_params=True))\n",
    "\n",
    "```\n",
    "\n",
    "Through the content of this notebook, we have been able to understand the concept of sharding and its implementation in FSDP. Let's move on to the next notebook to learn `Model parallelism`. To proceed, please click the [Next Link](model-parallelism.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8c2b7-66a3-4a8d-92e2-6896f78d52e8",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst\n",
    "- https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html\n",
    "- https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3c60b-25bb-4be8-b648-5476d82afeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
